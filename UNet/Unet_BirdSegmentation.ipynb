{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "757cf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61492f78",
   "metadata": {},
   "source": [
    "# Lege ein BirdDataset an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53f49818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, image_paths, image_dir, segmentation_dir, transform_image, transform_mask):\n",
    "        super(BirdDataset, self).__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.segmentation_dir = segmentation_dir\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_mask = transform_mask\n",
    "        with open(image_paths, 'r') as f:\n",
    "            self.images_paths = [line.split(\" \")[-1] for line in f.readlines()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_name = \".\".join(self.images_paths[index].split('.')[:-1])\n",
    "\n",
    "        image = Image.open(os.path.join(self.image_dir, f\"{image_name}.jpg\")).convert(\"RGB\")\n",
    "        seg = Image.open(os.path.join(self.segmentation_dir, f\"{image_name}.png\")).convert(\"L\")\n",
    "\n",
    "        image = self.transform_image(image)\n",
    "        seg = self.transform_mask(seg)\n",
    "\n",
    "        return image, seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bafa87",
   "metadata": {},
   "source": [
    "# Lege jetzt eine Funktion an, \n",
    "die das Datenset in Trainings- und Validierungsdaten aufteilt und die Daten in Batches liefert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fc8a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def load_data_set(image_paths, image_dir, segmentation_dir, transforms, batch_size=8, shuffle=True):\n",
    "    dataset = BirdDataset(image_paths,\n",
    "                          image_dir,\n",
    "                          segmentation_dir,\n",
    "                          transform_image=transforms[0],\n",
    "                          transform_mask=transforms[1])\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [11772, 16])\n",
    "\n",
    "    return DataLoader( train_dataset, batch_size=batch_size, shuffle=shuffle), DataLoader( \n",
    "                         val_dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83dc24",
   "metadata": {},
   "source": [
    "# Aufbau der UNet-Architektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db558d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb932ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        #print(\"x.shape=\", x.shape, \" p.shape=\", p.shape)\n",
    "        return x, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fed3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        #print(\"x dim = \", x.shape, \"  skip = \", skip.shape)\n",
    "        if x.shape != skip.shape:\n",
    "            x = TF.resize(x, size=skip.shape[2:])\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        #print(x.shape)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "536f382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(3, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "        #print(\"bottleneck dim = \", b.shape)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        #print(\"d1 dim = \", d1.shape)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        #print(\"d2 dim = \", d2.shape)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        #print(\"d3 dim = \", d3.shape)\n",
    "        #print(\"s1 dim = \", s1.shape)\n",
    "        d4 = self.d4(d3, s1)\n",
    "        #print(\"d4 dim = \", d4.shape)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        #print(\"outputs dim = \", outputs.shape)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4052d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    image = torch.randn((32, 3, 161, 161))\n",
    "    model = UNet()\n",
    "    out = model(image)\n",
    "    print(image.shape, out.shape)\n",
    "    assert out.shape == (32, 1, 161, 161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f336ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klaus/miniconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 161, 161]) torch.Size([32, 1, 161, 161])\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "163a19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 16,\n",
    "    \"image_dir\": \"CUB_200_2011/CUB_200_2011/images\",\n",
    "    \"segmentation_dir\": \"CUB_200_2011/CUB_200_2011/segmentations\",\n",
    "    \"image_paths\": \"CUB_200_2011/CUB_200_2011/images.txt\",\n",
    "    \"epochs\": 10,\n",
    "    \"checkpoint\": \"checkpoint/bird_segmentation_v1.pth\",\n",
    "    \"optimiser\": \"checkpoint/bird_segmentation_v1_optim.pth\",\n",
    "    \"continue_train\": False,\n",
    "    #\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \"device\" : \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13751564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transforms_image = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0., 0., 0.), (1., 1., 1.))\n",
    "])\n",
    "\n",
    "transforms_mask = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.,), (1.,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "597dfc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 736 batches\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = load_data_set(\n",
    "    config['image_paths'],\n",
    "    config['image_dir'],\n",
    "    config['segmentation_dir'],\n",
    "    transforms=[transforms_image, transforms_mask],\n",
    "    batch_size=config['batch_size']\n",
    ")\n",
    "\n",
    "print(\"loaded\", len(train_dataset), \"batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f04c2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(config['device'])\n",
    "optimiser = torch.optim.Adam(params=model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ba22f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['continue_train']:\n",
    "    state_dict = torch.load(config['checkpoint'])\n",
    "    optimiser_state = torch.load(config['optimiser'])\n",
    "    model.load_state_dict(state_dict)\n",
    "    optimiser.load_state_dict(optimiser_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e53c995d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (e1): encoder_block(\n",
       "    (conv): conv_block(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (e2): encoder_block(\n",
       "    (conv): conv_block(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (e3): encoder_block(\n",
       "    (conv): conv_block(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (e4): encoder_block(\n",
       "    (conv): conv_block(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (b): conv_block(\n",
       "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (d1): decoder_block(\n",
       "    (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): conv_block(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (d2): decoder_block(\n",
       "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): conv_block(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (d3): decoder_block(\n",
       "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): conv_block(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (d4): decoder_block(\n",
       "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): conv_block(\n",
       "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (outputs): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "#scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0b8f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_and_save(model, optimiser, epoch):\n",
    "    torch.save(model.state_dict(), config['checkpoint'])\n",
    "    torch.save(optimiser.state_dict(), config['optimiser'])\n",
    "\n",
    "    num_correct = 0\n",
    "    num_pixel = 0\n",
    "    dice_score = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dataset:\n",
    "            x = x.to(config['device'])\n",
    "            y = y.to(config['device'])\n",
    "\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixel += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds + y).sum() + 1e-8\n",
    "            )\n",
    "\n",
    "            torchvision.utils.save_image(preds, f\"test/pred/{epoch}.png\")\n",
    "            torchvision.utils.save_image(y, f\"test/true/{epoch}.png\")\n",
    "\n",
    "    print(\n",
    "        f\"Dice Score = {dice_score/len(val_dataset)}\"\n",
    "    )\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "375b8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "\n",
    "def train():\n",
    "    step = 0\n",
    "    for epoch in range(config['epochs']):\n",
    "        loop = tqdm(train_dataset)\n",
    "        for image, seg in loop:\n",
    "            image = image.to(config['device'])\n",
    "            seg = seg.float().to(config['device'])\n",
    "\n",
    "            #with torch.backends.mps.amp.autocast():\n",
    "            #with torch.autocast(device_type=\"mps\"):\n",
    "            pred = model(image)\n",
    "            loss = loss_fn(pred, seg)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward();\n",
    "            optimiser.step();\n",
    "            #scaler.scale(loss).backward()\n",
    "            #scaler.step(optimiser)\n",
    "            #scaler.update()\n",
    "\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            step += 1\n",
    "        check_accuracy_and_save(model, optimiser, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433aea2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 736/736 [12:55<00:00,  1.05s/it, loss=0.0337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Score = 0.923539400100708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 736/736 [13:06<00:00,  1.07s/it, loss=0.0386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Score = 0.9237179756164551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 736/736 [13:13<00:00,  1.08s/it, loss=0.0313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Score = 0.9237039089202881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|███████████████████████████▊| 730/736 [12:56<00:06,  1.07s/it, loss=0.0304]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "225f428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,13):\n",
    "    image_name = 'Bild' + '{:02d}'.format(i)\n",
    "    image = Image.open(os.path.join('./test', f\"{image_name}.png\")).convert(\"RGB\")\n",
    "    image = transforms_image(image)\n",
    "    image = image.to(config['device'])\n",
    "    image = image.reshape(1,3,256,256)\n",
    "    preds = torch.sigmoid(model(image))\n",
    "    torchvision.utils.save_image(preds, f\"test/pred/{image_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c704601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
